{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb73104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b99080",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = output.contiguous().view(-1, self.hidden_size)  # (batch_size*sequence_length, hidden_size)\n",
    "        output = self.fc(output)  # (batch_size*sequence_length, num_classes)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device),\n",
    "                weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3139e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_length=200, batch_size=64):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.text[idx:idx+self.seq_length]\n",
    "        label = self.text[idx+1:idx+self.seq_length+1]\n",
    "    \n",
    "        return char_tensor(sequence), char_tensor(label)\n",
    "\n",
    "# assuming that `text.txt` is your file and it's located at the current directory.\n",
    "with open('shakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "all_characters = sorted(list(set(text)))\n",
    "input_size = len(all_characters)\n",
    "output_size = len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "030d05f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [0/8713], Loss: 4.1690\n",
      "Epoch [1/10], Step [1000/8713], Loss: 1.5223\n",
      "Epoch [1/10], Step [2000/8713], Loss: 1.3995\n",
      "Epoch [1/10], Step [3000/8713], Loss: 1.3423\n",
      "Epoch [1/10], Step [4000/8713], Loss: 1.2957\n",
      "Epoch [1/10], Step [5000/8713], Loss: 1.2988\n",
      "Epoch [1/10], Step [6000/8713], Loss: 1.2670\n",
      "Epoch [1/10], Step [7000/8713], Loss: 1.2440\n",
      "Epoch [1/10], Step [8000/8713], Loss: 1.2520\n",
      "Epoch [2/10], Step [0/8713], Loss: 1.2328\n",
      "Epoch [2/10], Step [1000/8713], Loss: 1.2035\n",
      "Epoch [2/10], Step [2000/8713], Loss: 1.2148\n",
      "Epoch [2/10], Step [3000/8713], Loss: 1.1872\n",
      "Epoch [2/10], Step [4000/8713], Loss: 1.2046\n",
      "Epoch [2/10], Step [5000/8713], Loss: 1.1801\n",
      "Epoch [2/10], Step [6000/8713], Loss: 1.1825\n",
      "Epoch [2/10], Step [7000/8713], Loss: 1.1859\n",
      "Epoch [2/10], Step [8000/8713], Loss: 1.1627\n",
      "Epoch [3/10], Step [0/8713], Loss: 1.1686\n",
      "Epoch [3/10], Step [1000/8713], Loss: 1.1354\n",
      "Epoch [3/10], Step [2000/8713], Loss: 1.1485\n",
      "Epoch [3/10], Step [3000/8713], Loss: 1.1558\n",
      "Epoch [3/10], Step [4000/8713], Loss: 1.1692\n",
      "Epoch [3/10], Step [5000/8713], Loss: 1.1245\n",
      "Epoch [3/10], Step [6000/8713], Loss: 1.1551\n",
      "Epoch [3/10], Step [7000/8713], Loss: 1.1461\n",
      "Epoch [3/10], Step [8000/8713], Loss: 1.1179\n",
      "Epoch [4/10], Step [0/8713], Loss: 1.1103\n",
      "Epoch [4/10], Step [1000/8713], Loss: 1.0907\n",
      "Epoch [4/10], Step [2000/8713], Loss: 1.1477\n",
      "Epoch [4/10], Step [3000/8713], Loss: 1.1240\n",
      "Epoch [4/10], Step [4000/8713], Loss: 1.1002\n",
      "Epoch [4/10], Step [5000/8713], Loss: 1.1034\n",
      "Epoch [4/10], Step [6000/8713], Loss: 1.0996\n",
      "Epoch [4/10], Step [7000/8713], Loss: 1.0916\n",
      "Epoch [4/10], Step [8000/8713], Loss: 1.1216\n",
      "Epoch [5/10], Step [0/8713], Loss: 1.0742\n",
      "Epoch [5/10], Step [1000/8713], Loss: 1.0994\n",
      "Epoch [5/10], Step [2000/8713], Loss: 1.0974\n",
      "Epoch [5/10], Step [3000/8713], Loss: 1.1194\n",
      "Epoch [5/10], Step [4000/8713], Loss: 1.1023\n",
      "Epoch [5/10], Step [5000/8713], Loss: 1.0917\n",
      "Epoch [5/10], Step [6000/8713], Loss: 1.1043\n",
      "Epoch [5/10], Step [7000/8713], Loss: 1.0903\n",
      "Epoch [5/10], Step [8000/8713], Loss: 1.1025\n",
      "Epoch [6/10], Step [0/8713], Loss: 1.0911\n",
      "Epoch [6/10], Step [1000/8713], Loss: 1.0809\n",
      "Epoch [6/10], Step [2000/8713], Loss: 1.0691\n",
      "Epoch [6/10], Step [3000/8713], Loss: 1.0991\n",
      "Epoch [6/10], Step [4000/8713], Loss: 1.0992\n",
      "Epoch [6/10], Step [5000/8713], Loss: 1.0930\n",
      "Epoch [6/10], Step [6000/8713], Loss: 1.0579\n",
      "Epoch [6/10], Step [7000/8713], Loss: 1.0979\n",
      "Epoch [6/10], Step [8000/8713], Loss: 1.0591\n",
      "Epoch [7/10], Step [0/8713], Loss: 1.0844\n",
      "Epoch [7/10], Step [1000/8713], Loss: 1.1007\n",
      "Epoch [7/10], Step [2000/8713], Loss: 1.0504\n",
      "Epoch [7/10], Step [3000/8713], Loss: 1.0918\n",
      "Epoch [7/10], Step [4000/8713], Loss: 1.0754\n",
      "Epoch [7/10], Step [5000/8713], Loss: 1.0701\n",
      "Epoch [7/10], Step [6000/8713], Loss: 1.0828\n",
      "Epoch [7/10], Step [7000/8713], Loss: 1.0725\n",
      "Epoch [7/10], Step [8000/8713], Loss: 1.0814\n",
      "Epoch [8/10], Step [0/8713], Loss: 1.0661\n",
      "Epoch [8/10], Step [1000/8713], Loss: 1.0828\n",
      "Epoch [8/10], Step [2000/8713], Loss: 1.0695\n",
      "Epoch [8/10], Step [3000/8713], Loss: 1.0572\n",
      "Epoch [8/10], Step [4000/8713], Loss: 1.0610\n",
      "Epoch [8/10], Step [5000/8713], Loss: 1.0847\n",
      "Epoch [8/10], Step [6000/8713], Loss: 1.0774\n",
      "Epoch [8/10], Step [7000/8713], Loss: 1.0690\n",
      "Epoch [8/10], Step [8000/8713], Loss: 1.0590\n",
      "Epoch [9/10], Step [0/8713], Loss: 1.0433\n",
      "Epoch [9/10], Step [1000/8713], Loss: 1.0535\n",
      "Epoch [9/10], Step [2000/8713], Loss: 1.0626\n",
      "Epoch [9/10], Step [3000/8713], Loss: 1.0588\n",
      "Epoch [9/10], Step [4000/8713], Loss: 1.0701\n",
      "Epoch [9/10], Step [5000/8713], Loss: 1.0810\n",
      "Epoch [9/10], Step [6000/8713], Loss: 1.0555\n",
      "Epoch [9/10], Step [7000/8713], Loss: 1.0392\n",
      "Epoch [9/10], Step [8000/8713], Loss: 1.0619\n",
      "Epoch [10/10], Step [0/8713], Loss: 1.0665\n",
      "Epoch [10/10], Step [1000/8713], Loss: 1.0553\n",
      "Epoch [10/10], Step [2000/8713], Loss: 1.0618\n",
      "Epoch [10/10], Step [3000/8713], Loss: 1.0614\n",
      "Epoch [10/10], Step [4000/8713], Loss: 1.0577\n",
      "Epoch [10/10], Step [5000/8713], Loss: 1.0672\n",
      "Epoch [10/10], Step [6000/8713], Loss: 1.0727\n",
      "Epoch [10/10], Step [7000/8713], Loss: 1.0662\n",
      "Epoch [10/10], Step [8000/8713], Loss: 1.0599\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 126\n",
    "hidden_size = 128\n",
    "n_layers = 2\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10\n",
    "print_every = 1000\n",
    "plot_every = 100\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = TextDataset(text)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CharRNN(input_size, hidden_size, output_size, n_layers).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Track the training loss over epochs\n",
    "all_losses = []\n",
    "\n",
    "# Set the start hidden state\n",
    "hidden = model.init_hidden(128)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    #hidden = model.init_hidden(batch_size)\n",
    "    for i, (input_batch, target_batch) in enumerate(itertools.islice(dataloader, len(dataloader)-1)):\n",
    "            \n",
    "        input_batch = input_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())  # Detach the hidden state\n",
    "\n",
    "        output, hidden = model(input_batch, hidden)\n",
    "\n",
    "        loss = criterion(output.view(-1, output_size), target_batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' % (epoch, n_epochs, i, len(dataloader), loss.item()))\n",
    "\n",
    "    all_losses.append(total_loss / len(dataloader))\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plt.plot(all_losses)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.show()\n",
    "\n",
    "# Plot the final training loss\n",
    "plt.plot(all_losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a38dda24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBQLA3aketuch forth.\n",
      "Your best wingatager: they loose this delight\n",
      "A grave yourself; teach upon thy foesing roar with us.\n",
      "\n",
      "BIONDELLO:\n",
      "Where issues that vows for my great night at the daughters!\n",
      "Sorran he is, will I such, for nor ears,\n",
      "Or in that debt of all all cheek of it:\n",
      "If ever I be, if the potily!\n",
      "\n",
      "CAMILLO:\n",
      "It shall not have an enemy, with war;\n",
      "Or hath stop not your lordship, if I take for overhas's labour,\n",
      "And have been more woes looks asleep.'\n",
      "And tell me, and no other have I scatch.\n",
      "\n",
      "TRANIO:\n",
      "Year,' quote doth the dear,\n",
      "Or pluck'd their corps of pecrities\n",
      "That I lie the death of storeles of his\n",
      "homen of is mourning Cartse love,\n",
      "O' the subsider means their joysell!\n",
      "Thrushed vimous ere no other blessed dukel:\n",
      "But with sacundchood remedy, would be\n",
      "be power is thee withord. The beggar ever, draw\n",
      "are hold\n",
      "Should be offern'd lions country, ground\n",
      "Before you know them both.\n",
      "\n",
      "Second O shall fetch pursuit trouble your trie to know:\n",
      "Henceforwh, the very men,\n",
      "Despised and not both I'll crutyless of my stall, old:\n",
      "Is your parting by deny my poor knaves.\n",
      "\n",
      "CLAUDIO:\n",
      "\n",
      "ISALE:\n",
      "Well said,\n",
      "Is not to drink,--\n",
      "\n",
      "ANGELO:\n",
      "Ha! what archaries, that does with my brother!\n",
      "I mean not moon, himself see to prove your curses!\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Bless Hatelanl, then, for they think to tell thee,\n",
      "Executiful hearts of this sensible of yours;\n",
      "In my virtual daughter will get at mine:\n",
      "Rumult high rebels well my father: thence is my daughter\n",
      "feitony of you: any multe her sorrow;\n",
      "And, yew from our governor in his state,\n",
      "To you was amis but liberty had so, and he come\n",
      "For Suclitioon proof false; meaning\n",
      "What goodness, you prove; a bootlock'st for thy grace.\n",
      "Then, look you knows the king?\n",
      "\n",
      "ANGELO:\n",
      "I the noble living I can.\n",
      "\n",
      "SEBASTIAN:\n",
      "What then in the dave doth\n",
      "A crown came of my wedding pair; you\n",
      "shall faith fares you will: not be York to put\n",
      "How must hence, Tyrarals and earl at the base prepared\n",
      "Since it cannot tell. But grave in being person hath appear,\n",
      "And thus taken to strike, he hath profeds no tune:\n",
      "My pursty spring firm old conseturn swith instain,\n",
      "To mould; about him shakes his right.\n",
      "\n",
      "ROMEO:\n",
      "That, I hear a bear: maniffense struck nurse:\n",
      "Yet how know our general, that he hath.\n",
      "\n",
      "BRUTUS:\n",
      "Where prevaying so, sing, that know a beggard of our counteance, there be too late.\n",
      "He's her, of controoping kill'd my ground, for sharp of God she\n",
      "That neighbour, manif music more honour:\n",
      "A dian:\n",
      "I will back my traitor beford, was does\n",
      "Whan I did see you tender foelignions,\n",
      "But I would so young and lords; but if faction hold the answer:\n",
      "Where! how mark your wars? what may I had you make glad!\n",
      "And fildeed, we, and my most household man\n",
      "In valse,\n",
      "Were it, and hairy, with I bought to woo'd.\n",
      "\n",
      "ADRIAN:\n",
      "I shall say, better not to recomble of mine!\n",
      "Thy duke upon nd high as this.\n",
      "\n",
      "CAPULET:\n",
      "For meant fool.\n",
      "The vessed had in loving reasons,\n",
      "All but remembrance the drum.\n",
      "\n",
      "ANGELO:\n",
      "In this at any man I, ill beautiale.\n",
      "Yet show much moder that fame knows from canny.\n",
      "Ungentlemen, come, Gloucesters, we were with the ignorance\n",
      "As sits where you should be slain to thee;\n",
      "And thou shalt not be drun.\n",
      "\n",
      "BUCHIS:\n",
      "Where by this?\n",
      "Once more grief off it is not with him!\n",
      "And let the dath and men such sort as\n",
      "swallows that speaky converses of your return!\n",
      "And hither doth fresh embassed in this brows\n",
      "Cave for treation, when he is wonders fears\n",
      "Of that brase veins to knoct me here violent,\n",
      "The one her terridity is not the commonweal\n",
      "Of hours the lady should have sake in thy father's court:\n",
      "Richard not were Mercutio's death abroad!\n",
      "\n",
      "FLORIZEL:\n",
      "Where march'd your side? answer them; I loves his petitiently trunk\n",
      "From electively with him, on her brothers,\n",
      "Something to unfortunding to her oath:\n",
      "Richarler,--this after high services waked our mind\n",
      "When it might oppose thee with him in the\n",
      "opluir.' I heard murder me an orish of your necessobereance forth,\n",
      "My brother Coriold cleckle very words,\n",
      "And not unto melts, your mother true bridaged:\n",
      "For thereful defect of tyranny\n",
      "To watch the prisum for me.\n",
      "\n",
      "POMPEY:\n",
      "I never have a citizen are to him.\n",
      "But ye ancousand in elege.\n",
      "\n",
      "MARCIUS:\n",
      "Though thou speak'st thus most buy found's injury.\n",
      "And to the English cutting me; and his\n",
      "trencher than death. What't thou canst do love these glorious words,\n",
      "And let louce into a sea fearing as cut;\n",
      "Away botrought to come to Katharinam\n",
      "Show'st thou obedy.\n",
      "\n",
      "ALUMIO:\n",
      "I prittenfer you at the earnce to death,\n",
      "Where since their son I am a poor grins;\n",
      "When I arish thee for this.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "What faint I am just\n",
      "This shurds behote to your triblies? with such dying makes\n",
      "butt joyful bow, hear this night; whoneat's love!\n",
      "At their known be looked at Barneating,\n",
      "Of a minister of a survoulary of mine.\n",
      "\n",
      "BIONDELLO:\n",
      "Ax long-appear about thee, in both then,\n",
      "As my trouch breathing traitor while.\n",
      "\n",
      "ROMEO:\n",
      "Sir, purpose, me we too.\n",
      "\n",
      "MENENIUS:\n",
      "Go, a gaver.\n",
      "\n",
      "PETRUCHIO:\n",
      "I, I kepor your wings; I speak't a forfeignius: where;\n",
      "Right noble and the citize and now\n",
      "Destruction off fall first to make him so? O, you, love\n",
      "Shall his nose forth the\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Set the sequence length\n",
    "sequence_length = 5000\n",
    "\n",
    "# Initialize the hidden state\n",
    "hidden = model.init_hidden(1)\n",
    "hidden = tuple(h.to(device) for h in hidden)  # Move the hidden state to the same device as the model\n",
    "\n",
    "# Choose a starting character or seed\n",
    "start_char = 'C'\n",
    "\n",
    "# Convert the starting character to a tensor\n",
    "input_tensor = char_tensor(start_char).unsqueeze(0).to(device)  # Move the input tensor to the same device as the model\n",
    "\n",
    "# Create an empty list to store the generated sequence\n",
    "generated_sequence = []\n",
    "\n",
    "# Loop through each step of sequence generation\n",
    "for _ in range(sequence_length):\n",
    "    # Pass the input tensor and hidden state to the model\n",
    "    output, hidden = model(input_tensor, hidden)\n",
    "\n",
    "    # Move the output tensor to the CPU\n",
    "    output = output.cpu()\n",
    "\n",
    "    # Sample the next character from the output distribution\n",
    "    next_char_tensor = torch.multinomial(torch.exp(output.view(-1)), 1)\n",
    "    next_char = all_characters[next_char_tensor.item()]\n",
    "\n",
    "    # Append the sampled character to the generated sequence\n",
    "    generated_sequence.append(next_char)\n",
    "\n",
    "    # Set the input character for the next step as the sampled character\n",
    "    input_tensor = char_tensor(next_char).unsqueeze(0).to(device)  # Move the input tensor to the same device as the model\n",
    "\n",
    "# Join the generated sequence into a string\n",
    "generated_sequence = ''.join(generated_sequence)\n",
    "\n",
    "# Print the generated sequence\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f7d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
